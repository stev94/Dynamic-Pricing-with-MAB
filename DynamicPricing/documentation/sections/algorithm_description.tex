\newpage
\section{Algorithm Description}\label{sec:algorithm-description}

The algorithms used in the project have been divided into 2 main big sections,
the stationary case and the non stationary case. In both cases the main goal is to find the price per hour that maximizes
the profit.

The algorithms will learn the demand curve in some points, one for each arm.
For the Bandits algorithm, decision of which arm to pull will be based on the expected mean provided by the arm,
multiplied by the price. This is because each arm will estimate the conversion rate of the demand curve for its relative price.

\subsection{Stationary case}\label{subsec:stationary-case}

In the stationary case we have 4 algorithms: Sequential AB testing, UCB1,
Thompson Sampling and a Greedy algorithm (used as a baseline). UCB1 and Thompson sampling are bandit algorithms,
while AB testing isn't (we could also say that UCB1 and TS are online algorithms, while AB testing is an offline algorithm).
What this means is that the AB testing is strictly divided into phases of exploration and phases of exploitation, while the bandit algorithms mix exploration with exploitation.

These algorithms share some parameters: The time horizon, that is the number of observations of one single experiment,
and the number of times the experiment is repeated. In fact, since we are simulating the environment we want to reduce the
noise of the random variables that corresponds to the users in our simulation.

\subsubsection{Sequential AB testing}

The sequential AB testing is perhaps the easiest of the algorithms (excluding the greedy algorithm), but at the same time,
as shown by the results below, the one that performs the worst.
Tuning the parameters for the algorithm is extremely important to get good results.
AB testing, as stated above, is divided into very clear phases of exploration, in which we continue making consistent choices,
even if they are non optimal, until we have reached a certain number of samples, at which point we make the best choice and
proceed to exploit it. In sequential AB testing, the exploration takes a while longer, as we need to compare more times.
Once we have found the optimal arm, we enter the exploitation phase.

The algorithm consists of identifying the set of arms. From those arms, chose 2 at random.
Then for a given fixed number of comparisons randomly pull 1 of the 2 arms, and save the rewards.
After the fixed number of comparisons is done, compare the results of the 2 arms, and if we can say with a certain
degree of confidence that one of the arms is better than the other, we discard the worse arm, put back the better arm
into the poll of arms, and repeat the process until 1 arm remains, or the time horizon expires.
If we can't say that one arm is better than the other based on these observations, we put both arms back in the poll and
restart the process with 2 new arms. As a consequence of the behaviour of the algorithm, it is possible that by the end of the
experiment, there is still more than 1 arm in the arms poll. If that were to be the case, it could mean that the number of
comparisons between to arms is not high enough, or the time horizion is not big enough to compare every arm at least once
(i.e 400 comparisons per pair of arms, 1600 time horizon, but 8 arms), or that the value of the accuracy is too small.

Given this behaviour there are 2 parameters that need to be set: the number of times we compare between 2 arms, and the
total time horizon for the experiment.



\subsubsection{Greedy}

The greedy algorithm consists of just pulling the arm the looks the best, without any form of learning.
It will always pull the arm with the best expected value, so we can say that this algorithm is only exploitation.
In fact it perform well only when the best arm is really easy to find. We use it as a baseline for the other Bandits algorithm,
since it is the simplest one. As will be shown later, it is the worst performing algorithm.


\subsubsection{UCB1}

The UCB1 algorithm is a Bandit algorithm, and in it's core is very simple. UCB stands for Upper Confidence Bound, and it is what
defines the algorithm. The idea is to prefer exploration in case of uncertainty.

The algorithm starts by pulling each arm once. After then, it will always pull the arm with the highest upper confidence bound.
In this way if there is one arm that is very good it will have an high mean, but probably also a small upper bound since the
algorithm will have pulled it a lot of times. While if an arm has performed poorly but it has been pulled just a few time,
it will have an high upper bound and so it will still have the possibility to be choose.

For calculating the bound we use the Hoeffding bound, that provides an upper bound on the probability that the sum of bounded
independent random variables deviates from its expected value by more than a certain amount c.
The upper bound we use is the following:
\[c*\sqrt{\frac{log{t}}{n_a(t-1)}}\]
where t is the current time, $n_a(t-1)$ is the number of times the arm has been pulled at t-1, c is the
exploration-exploitation parameter.
The parameter c is very important since it defines the degree of exploration of the algorithm: the higher c is,
the more exploration will have the algorithm. We experiments with different values, in particular 1, 2, $\sqrt{2}$,
and the best performance was achieved with c equal to 1. This can be explain by the fact that the demand curve for
the price per hour of a car is normally  monotonic and it's enough easy to identify the best price. So the algorithm
does not need a lot of exploration in this case.


\subsubsection{Thompson Sampling}

Thompsons Sampling is another bandit algorithm, that instead use a Bayesian approach to find the best price.
For every arm we have a prior on it's expected value. Since the arms are distributed as Bernoulli variables the priors are
distributed as Betas, starting with both alpha and beta both equal to 1; in this way we start the algorithm with a uniform
distribution for each price with mean 0.5.

The algorithm then proceeds as follows: for every arm we draw a sample according to the associated Beta distribution,
we then chose the arm with the best sampled value multiplied by its price, and we pull it.
Finally with the obtained reward we update the values of the Beta distribution of the chosen arm according to the following formula:
\[(\alpha, \beta) := (\alpha, \beta) + (reward, 1 - reward)\] 

In this case the degree of exploration is given by the prior Beta distribution of each arm. In particular the Thompson Sampling
algorithm will have a good estimate of the expected mean of only the best arms with respect to the UCB1.
We can notice that as we draw more and more values for one arm, the parameters of the Beta start to indicate which is the best
arm in the scenario.

\subsection{Non-Stationary case}\label{subsec:non-stationary-case}

In the non-stationary case we have 2 algorithms: Non stationary UCB1 and Non stationary Thompson Sampling.
In this case we are assuming that during time the demand curve can change, and so the algorithm must decide the best arm to pull
by taking only the last observations. To simulate it we use a Sliding Window, that will slide over the observations of each arm,
and each arm will decide the best arm by taking only the observations that are inside the window.
The sliding window moves at each step, and forgets the oldest value.
 
A very important parameter to set in the non-stationary algorithms is the length of the sliding window,
as it will heavily impact the functioning of the algorithm.
The selection must take into account the problem we are solved and must approximate as much as possible the true length of
each phase to have a good performance on the algorithm. It must be also enough big to contain enough observations so
the algorithms can learn something, i.e.\ the sliding window must be set to at least the minimum time horizon that needs
the stationary algorithms to perform well.


\subsubsection{NS-UCB1 and NS-TS}

As stated above, the algorithms perform exactly as in the non stationary case.
The only big difference is the amount of data we use to calculate the Upper Confidence Bounds and the Beta distributions.
That is, once a value is outside the sliding window, it has to be excluded from the calculation of the upper confidence
bound and the calculation of the Beta parameters. By doing this, we are able to forget old results,
which may not be providing correct information anymore.

The bound used to calculate the best arm in the UCB1 is the almost same as before, with the difference that as the current
time horizon we select always the minimum between t and the windows size.